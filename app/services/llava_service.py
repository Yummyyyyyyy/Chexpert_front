"""
LLaVAæ¨¡å‹æœåŠ¡
è°ƒç”¨è‡ªç ”å¤šæ¨¡æ€æ¨¡å‹ç”ŸæˆåŒ»å­¦æŠ¥å‘Š
"""
from loguru import logger
from typing import Tuple
import time
import os


class LLaVAService:
    """
    LLaVAæ¨¡å‹è°ƒç”¨æœåŠ¡

    ã€TODO - åç«¯å›¢é˜Ÿæˆå‘˜éœ€è¦å®ç°ã€‘:
    1. å®ç°çœŸå®çš„LLaVAæ¨¡å‹æ¨ç†é€»è¾‘
    2. ä¼˜åŒ–æç¤ºè¯æ¨¡æ¿ï¼Œæé«˜æŠ¥å‘Šè´¨é‡
    3. æ·»åŠ æŠ¥å‘Šåå¤„ç†ï¼ˆæ ¼å¼åŒ–ã€å»é™¤é‡å¤ç­‰ï¼‰
    4. è€ƒè™‘æ·»åŠ æ‰¹é‡æ¨ç†æ”¯æŒ
    5. æ·»åŠ æ¨ç†ç¼“å­˜æœºåˆ¶
    """

    def __init__(self):
        # ã€TODOã€‘è¿™é‡Œå¯ä»¥åˆå§‹åŒ–æ¨¡å‹æˆ–åŠ è½½æç¤ºè¯æ¨¡æ¿
        self.prompt_template = """
ä½œä¸ºä¸€åä¸“ä¸šçš„æ”¾å°„ç§‘åŒ»ç”Ÿï¼Œè¯·ä»”ç»†åˆ†æè¿™å¼ èƒ¸éƒ¨Xå…‰ç‰‡ï¼Œå¹¶ç”Ÿæˆè¯¦ç»†çš„åŒ»å­¦æŠ¥å‘Šã€‚

è¯·åŒ…å«ä»¥ä¸‹å†…å®¹ï¼š
1. å½±åƒè§‚å¯Ÿï¼ˆImage Findingsï¼‰
2. ç—…å˜æè¿°ï¼ˆPathology Descriptionï¼‰
3. åˆæ­¥è¯Šæ–­ï¼ˆImpressionï¼‰
4. å»ºè®®ï¼ˆRecommendationsï¼‰

ç”¨æˆ·æç¤ºï¼š{user_prompt}
"""

    async def generate_report(
        self,
        image_path: str,
        prompt: str
    ) -> Tuple[str, float]:
        """
        ç”ŸæˆåŒ»å­¦æŠ¥å‘Š

        å‚æ•°:
            image_path: å›¾ç‰‡è·¯å¾„
            prompt: ç”¨æˆ·è‡ªå®šä¹‰æç¤ºè¯

        è¿”å›:
            (report, processing_time): æŠ¥å‘Šæ–‡æœ¬å’Œå¤„ç†æ—¶é—´

        ã€TODOã€‘åç«¯å›¢é˜Ÿæˆå‘˜å®ç°:
        ```python
        from transformers import LlavaForConditionalGeneration, AutoProcessor
        from PIL import Image

        # 1. åŠ è½½å›¾åƒ
        image = Image.open(image_path)

        # 2. å‡†å¤‡è¾“å…¥
        model = model_manager.get_llava_model()
        processor = AutoProcessor.from_pretrained(settings.LLAVA_MODEL_PATH)

        full_prompt = self.prompt_template.format(user_prompt=prompt)
        inputs = processor(text=full_prompt, images=image, return_tensors="pt")

        # 3. æ¨¡å‹æ¨ç†
        start_time = time.time()
        with torch.no_grad():
            outputs = model.generate(**inputs, max_new_tokens=512)
        processing_time = time.time() - start_time

        # 4. è§£ç è¾“å‡º
        report = processor.decode(outputs[0], skip_special_tokens=True)

        return report, processing_time
        ```
        """
        logger.warning("âš ï¸  LLaVAæŠ¥å‘Šç”Ÿæˆé€»è¾‘å¾…å®ç° (llava_service.py:71)")

        # ============ ä»¥ä¸‹æ˜¯æ¨¡æ‹Ÿä»£ç ï¼Œä¾›å‰ç«¯è°ƒè¯•ä½¿ç”¨ ============
        # ã€åç«¯å›¢é˜Ÿæˆå‘˜éœ€è¦æ›¿æ¢ä¸ºçœŸå®å®ç°ã€‘

        # æ£€æŸ¥å›¾ç‰‡æ˜¯å¦å­˜åœ¨
        if not os.path.exists(image_path):
            raise FileNotFoundError(f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")

        # æ¨¡æ‹Ÿæ¨ç†æ—¶é—´
        start_time = time.time()
        await self._simulate_inference()
        processing_time = time.time() - start_time

        # æ¨¡æ‹Ÿç”Ÿæˆçš„æŠ¥å‘Šï¼ˆã€TODOã€‘æ›¿æ¢ä¸ºçœŸå®æ¨¡å‹è¾“å‡ºï¼‰
        mock_report = f"""
**èƒ¸éƒ¨Xå…‰ç‰‡åˆ†ææŠ¥å‘Š**

ã€å½±åƒè§‚å¯Ÿã€‘
- å¿ƒå½±å¤§å°ï¼šè½»åº¦å¢å¤§ï¼Œå¿ƒèƒ¸æ¯”çº¦0.55ï¼ˆæ­£å¸¸<0.5ï¼‰
- è‚ºé‡ï¼šåŒè‚ºçº¹ç†å¢å¤šï¼Œå¯è§è‚ºæ·¤è¡€å¾è±¡
- è‚‹è†ˆè§’ï¼šåŒä¾§è‚‹è†ˆè§’æ¸…æ™°ï¼Œæœªè§ç§¯æ¶²
- éª¨éª¼ç»“æ„ï¼šæœªè§æ˜æ˜¾éª¨æŠ˜æˆ–éª¨è´¨ç ´å

ã€ç—…å˜æè¿°ã€‘
1. å¿ƒè„è‚¥å¤§ï¼ˆCardiomegalyï¼‰ï¼šå¿ƒå½±è½®å»“æ‰©å¤§ï¼Œæç¤ºå¯èƒ½å­˜åœ¨å¿ƒåŠŸèƒ½ä¸å…¨
2. è‚ºæ°´è‚¿ï¼ˆPulmonary Edemaï¼‰ï¼šåŒè‚ºå¯è§è¡€ç®¡å½±æ¨¡ç³Šï¼Œç¬¦åˆè‚ºæ·¤è¡€è¡¨ç°

ã€åˆæ­¥è¯Šæ–­ã€‘
1. å¿ƒè„è‚¥å¤§
2. è½»åº¦è‚ºæ°´è‚¿

ã€å»ºè®®ã€‘
1. å»ºè®®è¿›ä¸€æ­¥è¡Œå¿ƒè„è¶…å£°æ£€æŸ¥ï¼Œè¯„ä¼°å¿ƒåŠŸèƒ½
2. å®Œå–„è¡€å¸¸è§„ã€BNPç­‰æ£€æŸ¥
3. å»ºè®®å¿ƒå†…ç§‘ä¼šè¯Š

*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {processing_time:.2f}ç§’*
*æœ¬æŠ¥å‘Šç”±AIè¾…åŠ©ç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒï¼Œæœ€ç»ˆè¯Šæ–­è¯·ä»¥åŒ»ç”Ÿåˆ¤æ–­ä¸ºå‡†*

---
ç”¨æˆ·æç¤ºè¯: {prompt}
        """

        logger.info(f"ğŸ“ æ¨¡æ‹Ÿç”ŸæˆæŠ¥å‘Šï¼Œè€—æ—¶ {processing_time:.2f}ç§’")
        return mock_report.strip(), processing_time

    async def _simulate_inference(self):
        """æ¨¡æ‹Ÿæ¨ç†è€—æ—¶"""
        import asyncio
        await asyncio.sleep(1.5)  # æ¨¡æ‹ŸLLaVAæ¨ç†è€—æ—¶
